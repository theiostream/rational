{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1Fe4FNFHx23VIfet1AkxD8J95Mfqn_XlC",
      "authorship_tag": "ABX9TyM4iDwb9mOMo08nLYzZi2ZO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theiostream/rational/blob/master/CT2US.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CT2US\n",
        "\n",
        "This tool is intended to automate the generation of simulated ultrasound image and label pairs from ct images (.nii/.nii.gz).\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## WIP:\n",
        "- there will be a visualizer for the end (slices) and intermediary (segmentation) results\n",
        "- progress bar will be improved\n",
        "- code for two alternate optimized segmentation pipelines is still being developed\n",
        "  - one focusing on avoiding internal totalsegmentator steps being saved to memory\n",
        "  - another further optimizes by properly using gpu and cpu acceleration.\n",
        "    \n",
        "---\n",
        "Old version of the optimized versions present here, check for potential updates in https://github.com/lczamprogno/CT2US."
      ],
      "metadata": {
        "id": "UzmsGLmOGUvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title This needs to be run once and then the session needs to be restarted\n",
        "%pip install totalsegmentator dask numba cupy-cuda12x dask_cuda torchvision xmltodict torchio cucim \"bokeh>=3.1.0\" di gradio tensordict pathlib"
      ],
      "metadata": {
        "id": "8-C6Ik3Z48SC",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Necessary step to use gradio UI within notebook\n",
        "%load_ext gradio"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GihZqJqBC0LN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title IMPORTANT: Acquire a totalsegmentator key (https://backend.totalsegmentator.com/license-academic/) and set it in the line in this block marked by a comment\n",
        "import sys\n",
        "from pathlib import PosixPath as path\n",
        "import os\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import xmltodict\n",
        "from itertools import tee, islice\n",
        "\n",
        "from nibabel import nifti1\n",
        "from numpy import uint8\n",
        "\n",
        "import totalsegmentator.python_api as ts\n",
        "from totalsegmentator.config import setup_nnunet, setup_totalseg, set_config_key, get_weights_dir\n",
        "\n",
        "this_folder = path(\"../CT2US\").resolve()\n",
        "\n",
        "sys.path.append(this_folder)\n",
        "ts_cfg_path = path.joinpath(this_folder, \".totalsegmentator\")\n",
        "ts_cfg_path.mkdir(exist_ok=True, parents=True)\n",
        "os.environ[\"TOTALSEG_HOME_DIR\"] = str(ts_cfg_path)\n",
        "\n",
        "setup_nnunet()\n",
        "setup_totalseg()\n",
        "# TODO: Request and set the totalsegmentator license here\n",
        "ts.set_license_number(\"aca_IY2KSBZZUM5QZO\")\n",
        "from nnunetv2.inference.predict_from_raw_data import nnUNetPredictor\n",
        "from nnunetv2.utilities.file_path_utilities import get_output_folder\n",
        "\n",
        "from totalsegmentator.libs import download_model_with_license_and_unpack, download_url_and_unpack\n",
        "from totalsegmentator.map_to_binary import commercial_models\n",
        "\n",
        "from numba import jit, njit, cuda\n",
        "import cupy as cp\n",
        "import cupyx.scipy.ndimage as cusci\n",
        "import asyncio\n",
        "from dask.distributed import Client, LocalCluster\n",
        "from dask import delayed\n",
        "from dask.distributed import print as pr\n",
        "import dask.bag as db\n",
        "import dask\n",
        "import torch\n",
        "from torch.distributions import Normal\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from torchvision import transforms\n",
        "from math import pi\n",
        "\n",
        "\n",
        "from typing import List, Dict\n",
        "\n",
        "import torchvision.transforms.functional as F\n"
      ],
      "metadata": {
        "id": "9HH9ibpeNCYr",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup configs\n",
        "!wget \"https://drive.google.com/uc?export=download&id=1JqoSIrLnZt3Dfet3qTnUtPR77eFfckAb\"\n",
        "!unzip '/content/uc?export=download&id=1JqoSIrLnZt3Dfet3qTnUtPR77eFfckAb' -d '/content/CT2US'\n",
        "!rm '/content/uc?export=download&id=1JqoSIrLnZt3Dfet3qTnUtPR77eFfckAb'\n",
        "with open(path.joinpath(this_folder, \"configs\", \"name2label.json\")) as n:\n",
        "    name2label = dict(json.load(n))"
      ],
      "metadata": {
        "id": "IuNzKCGqmkFl",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title US slice simulation code (from https://github.com/danivelikova/lotus/blob/main/models/us_rendering_model.py)\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from torchvision import transforms\n",
        "from math import pi\n",
        "\n",
        "# 2 - lung; 3 - fat; 4 - vessel; 6 - kidney; 8 - muscle; 9 - background; 11 - liver; 12 - soft tissue; 13 - bone;\n",
        "# Default Parameters from: https://github.com/Blito/burgercpp/blob/master/examples/ircad11/liver.scene , labels 8, 9 and 12 approximated from other labels\n",
        "\n",
        "                     # indexes:           2       3     4     6     8      9     11    12    13\n",
        "acoustic_imped_def_dict = torch.tensor([0.0004, 1.38, 1.61,  1.62, 1.62,  0.3,  1.65, 1.63, 7.8], requires_grad=True).to(device='cuda')    # Z in MRayl\n",
        "attenuation_def_dict =    torch.tensor([1.64,   0.63, 0.18,  1.0,  1.09, 0.54,  0.7,  0.54, 5.0], requires_grad=True).to(device='cuda')    # alpha in dB cm^-1 at 1 MHz\n",
        "mu_0_def_dict =           torch.tensor([0.78,   0.5,  0.001, 0.45,  0.45,  0.3,  0.4, 0.45, 0.78], requires_grad=True).to(device='cuda') # mu_0 - scattering_mu   mean brightness\n",
        "mu_1_def_dict =           torch.tensor([0.56,   0.5,  0.0,   0.6,  0.64,  0.2,  0.8,  0.64, 0.56], requires_grad=True).to(device='cuda') # mu_1 - scattering density, Nr of scatterers/voxel\n",
        "sigma_0_def_dict =        torch.tensor([0.1,    0.0,  0.01,  0.3,  0.1,   0.0,  0.14, 0.1,  0.1], requires_grad=True).to(device='cuda') # sigma_0 - scattering_sigma - brightness std\n",
        "\n",
        "\n",
        "alpha_coeff_boundary_map = 0.1\n",
        "beta_coeff_scattering = 10  #100 approximates it closer\n",
        "TGC = 8\n",
        "CLAMP_VALS = True\n",
        "\n",
        "\n",
        "def gaussian_kernel(size: int, mean: float, std: float):\n",
        "    d1 = torch.distributions.Normal(mean, std)\n",
        "    d2 = torch.distributions.Normal(mean, std*3)\n",
        "    vals_x = d1.log_prob(torch.arange(-size, size+1, dtype=torch.float32)).exp()\n",
        "    vals_y = d2.log_prob(torch.arange(-size, size+1, dtype=torch.float32)).exp()\n",
        "\n",
        "    gauss_kernel = torch.einsum('i,j->ij', vals_x, vals_y)\n",
        "\n",
        "    return gauss_kernel / torch.sum(gauss_kernel).reshape(1, 1)\n",
        "\n",
        "g_kernel = gaussian_kernel(3, 0., 0.5)\n",
        "g_kernel = torch.tensor(g_kernel[None, None, :, :], dtype=torch.float32).to(device='cuda')\n",
        "\n",
        "\n",
        "class UltrasoundRendering(torch.nn.Module):\n",
        "    def __init__(self, params, default_param=False):\n",
        "        super(UltrasoundRendering, self).__init__()\n",
        "        self.params = params\n",
        "\n",
        "        if default_param:\n",
        "            self.acoustic_impedance_dict = acoustic_imped_def_dict.detach().clone()\n",
        "            self.attenuation_dict = attenuation_def_dict.detach().clone()\n",
        "            self.mu_0_dict = mu_0_def_dict.detach().clone()\n",
        "            self.mu_1_dict = mu_1_def_dict.detach().clone()\n",
        "            self.sigma_0_dict = sigma_0_def_dict.detach().clone()\n",
        "\n",
        "        else:\n",
        "            self.acoustic_impedance_dict = torch.nn.Parameter(acoustic_imped_def_dict)\n",
        "            self.attenuation_dict = torch.nn.Parameter(attenuation_def_dict)\n",
        "\n",
        "            self.mu_0_dict = torch.nn.Parameter(mu_0_def_dict)\n",
        "            self.mu_1_dict = torch.nn.Parameter(mu_1_def_dict)\n",
        "            self.sigma_0_dict = torch.nn.Parameter(sigma_0_def_dict)\n",
        "\n",
        "        self.labels = [\"lung\", \"fat\", \"vessel\", \"kidney\", \"muscle\", \"background\", \"liver\", \"soft tissue\", \"bone\"]\n",
        "\n",
        "        self.attenuation_medium_map, self.acoustic_imped_map, self.sigma_0_map, self.mu_1_map, self.mu_0_map  = ([] for i in range(5))\n",
        "\n",
        "\n",
        "    def map_dict_to_array(self, dictionary, arr):\n",
        "        mapping_keys = torch.tensor([2, 3, 4, 6, 8, 9, 11, 12, 13], dtype=torch.long).to(device='cuda')\n",
        "        keys = torch.unique(arr).to(device='cuda')\n",
        "\n",
        "        index = torch.where(mapping_keys[None, :] == keys[:, None])[1]\n",
        "        values = torch.gather(dictionary, dim=0, index=index)\n",
        "        values = values.to(device='cuda')\n",
        "        # values.register_hook(lambda grad: print(grad))    # check the gradient during training\n",
        "\n",
        "        mapping = torch.zeros(keys.max().item() + 1).to(device='cuda')\n",
        "        mapping[keys] = values\n",
        "        return mapping[arr]\n",
        "\n",
        "\n",
        "    def plot_fig(self, fig, fig_name, grayscale):\n",
        "        save_dir='results_test/'\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.mkdir(save_dir)\n",
        "\n",
        "        plt.clf()\n",
        "\n",
        "        if torch.is_tensor(fig):\n",
        "            fig = fig.cpu().detach().numpy()\n",
        "\n",
        "        if grayscale:\n",
        "            plt.imshow(fig, cmap='gray', vmin=0, vmax=1, interpolation='none', norm=None)\n",
        "        else:\n",
        "            plt.imshow(fig, interpolation='none', norm=None)\n",
        "        plt.axis('off')\n",
        "        plt.savefig(save_dir + fig_name + '.png', bbox_inches='tight',transparent=True, pad_inches=0)\n",
        "\n",
        "\n",
        "    def clamp_map_ranges(self):\n",
        "        self.attenuation_medium_map = torch.clamp(self.attenuation_medium_map, 0, 10)\n",
        "        self.acoustic_imped_map = torch.clamp(self.acoustic_imped_map, 0, 10)\n",
        "        self.sigma_0_map = torch.clamp(self.sigma_0_map, 0, 1)\n",
        "        self.mu_1_map = torch.clamp(self.mu_1_map, 0, 1)\n",
        "        self.mu_0_map = torch.clamp(self.mu_0_map, 0, 1)\n",
        "\n",
        "\n",
        "    def rendering(self, H, W, z_vals=None, refl_map=None, boundary_map=None):\n",
        "\n",
        "        dists = torch.abs(z_vals[..., :-1, None] - z_vals[..., 1:, None])     # dists.shape=(W, H-1, 1)\n",
        "        dists = dists.squeeze(-1)                                             # dists.shape=(W, H-1)\n",
        "        dists = torch.cat([dists, dists[:, -1, None]], dim=-1)                # dists.shape=(W, H)\n",
        "\n",
        "        attenuation = torch.exp(-self.attenuation_medium_map * dists)\n",
        "        attenuation_total = torch.cumprod(attenuation, dim=1, dtype=torch.float32, out=None)\n",
        "\n",
        "        gain_coeffs = np.linspace(1, TGC, attenuation_total.shape[1])\n",
        "        gain_coeffs = np.tile(gain_coeffs, (attenuation_total.shape[0], 1))\n",
        "        gain_coeffs = torch.tensor(gain_coeffs).to(device='cuda')\n",
        "        attenuation_total = attenuation_total * gain_coeffs     # apply TGC\n",
        "\n",
        "        reflection_total = torch.cumprod(1. - refl_map * boundary_map, dim=1, dtype=torch.float32, out=None)\n",
        "        reflection_total = reflection_total.squeeze(-1)\n",
        "        reflection_total_plot = torch.log(reflection_total + torch.finfo(torch.float32).eps)\n",
        "\n",
        "        texture_noise = torch.randn(H, W, dtype=torch.float32).to(device='cuda')\n",
        "        scattering_probability = torch.randn(H, W, dtype=torch.float32).to(device='cuda')\n",
        "\n",
        "        scattering_zero = torch.zeros(H, W, dtype=torch.float32).to(device='cuda')\n",
        "\n",
        "        z = self.mu_1_map - scattering_probability\n",
        "        sigmoid_map = torch.sigmoid(beta_coeff_scattering * z)\n",
        "\n",
        "        # approximating  Eq. (4) to be differentiable:\n",
        "        # where(scattering_probability <= mu_1_map,\n",
        "        #                     texture_noise * sigma_0_map + mu_0_map,\n",
        "        #                     scattering_zero)\n",
        "        scatterers_map =  (sigmoid_map) * (texture_noise * self.sigma_0_map + self.mu_0_map) + (1 -sigmoid_map) * scattering_zero   # Eq. (6)\n",
        "\n",
        "        psf_scatter_conv = torch.nn.functional.conv2d(input=scatterers_map[None, None, :, :], weight=g_kernel, stride=1, padding=\"same\")\n",
        "        psf_scatter_conv = psf_scatter_conv.squeeze()\n",
        "\n",
        "        b = attenuation_total * psf_scatter_conv    # Eq. (3)\n",
        "\n",
        "        border_convolution = torch.nn.functional.conv2d(input=boundary_map[None, None, :, :], weight=g_kernel, stride=1, padding=\"same\")\n",
        "        border_convolution = border_convolution.squeeze()\n",
        "\n",
        "        r = attenuation_total * reflection_total * refl_map * border_convolution # Eq. (2)\n",
        "\n",
        "        intensity_map = b + r   # Eq. (1)\n",
        "        intensity_map = intensity_map.squeeze()\n",
        "        intensity_map = torch.clamp(intensity_map, 0, 1)\n",
        "\n",
        "        return intensity_map, attenuation_total, reflection_total_plot, scatterers_map, scattering_probability, border_convolution, texture_noise, b, r\n",
        "\n",
        "\n",
        "    def render_rays(self, W, H):\n",
        "        N_rays = W\n",
        "        t_vals = torch.linspace(0., 1., H).to(device='cuda')   # 0-1 linearly spaced, shape H\n",
        "        z_vals = t_vals.unsqueeze(0).expand(N_rays , -1) * 4\n",
        "\n",
        "        return z_vals\n",
        "\n",
        "    # warp the linear US image to approximate US image from curvilinear US probe\n",
        "    def warp_img(self, inputImage):\n",
        "        resultWidth = 360\n",
        "        resultHeight = 220\n",
        "        centerX = resultWidth / 2\n",
        "        centerY = -120.0\n",
        "        maxAngle =  60.0 / 2 / 180 * pi #rad\n",
        "        minAngle = -maxAngle\n",
        "        minRadius = 140.0\n",
        "        maxRadius = 340.0\n",
        "\n",
        "        h, w = inputImage.squeeze().shape\n",
        "\n",
        "        import torch.nn.functional as F\n",
        "\n",
        "        # Create x and y grids\n",
        "        x = torch.arange(resultWidth).float() - centerX\n",
        "        y = torch.arange(resultHeight).float() - centerY\n",
        "        xx, yy = torch.meshgrid(x, y)\n",
        "\n",
        "        # Calculate angle and radius\n",
        "        angle = torch.atan2(xx, yy)\n",
        "        radius = torch.sqrt(xx ** 2 + yy ** 2)\n",
        "\n",
        "        # Create masks for angle and radius\n",
        "        angle_mask = (angle > minAngle) & (angle < maxAngle)\n",
        "        radius_mask = (radius > minRadius) & (radius < maxRadius)\n",
        "\n",
        "        # Calculate original column and row\n",
        "        origCol = (angle - minAngle) / (maxAngle - minAngle) * w\n",
        "        origRow = (radius - minRadius) / (maxRadius - minRadius) * h\n",
        "\n",
        "        # Reshape input image to be a batch of 1 image\n",
        "        inputImage = inputImage.float().unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        # Scale original column and row to be in the range [-1, 1]\n",
        "        origCol = origCol / (w - 1) * 2 - 1\n",
        "        origRow = origRow / (h - 1) * 2 - 1\n",
        "\n",
        "        # Transpose input image to have channels first\n",
        "        inputImage = inputImage.permute(0, 1, 3, 2)\n",
        "\n",
        "        # Use grid_sample to interpolate\n",
        "        grid = torch.stack([origCol, origRow], dim=-1).unsqueeze(0).to('cuda')\n",
        "        resultImage = F.grid_sample(inputImage, grid, mode='bilinear', align_corners=True)\n",
        "\n",
        "        # Apply masks and set values outside of mask to 0\n",
        "        resultImage[~(angle_mask.unsqueeze(0).unsqueeze(0) & radius_mask.unsqueeze(0).unsqueeze(0))] = 0.0\n",
        "        resultImage_resized = transforms.Resize((256,256))(resultImage).float().squeeze()\n",
        "\n",
        "        return resultImage_resized\n",
        "\n",
        "\n",
        "    def forward(self, ct_slice):\n",
        "        if self.params[\"debug\"]: self.plot_fig(ct_slice, \"ct_slice\", False)\n",
        "\n",
        "        #init tissue maps\n",
        "        #generate 2D acousttic_imped map\n",
        "        self.acoustic_imped_map = self.map_dict_to_array(self.acoustic_impedance_dict, ct_slice)#.astype('int64'))\n",
        "\n",
        "        #generate 2D attenuation map\n",
        "        self.attenuation_medium_map = self.map_dict_to_array(self.attenuation_dict, ct_slice)\n",
        "\n",
        "        if self.params[\"debug\"]:\n",
        "            self.plot_fig(self.acoustic_imped_map, \"acoustic_imped_map\", False)\n",
        "            self.plot_fig(self.attenuation_medium_map, \"attenuation_medium_map\", False)\n",
        "\n",
        "        self.mu_0_map = self.map_dict_to_array(self.mu_0_dict, ct_slice)\n",
        "\n",
        "        self.mu_1_map = self.map_dict_to_array(self.mu_1_dict, ct_slice)\n",
        "\n",
        "        self.sigma_0_map = self.map_dict_to_array(self.sigma_0_dict, ct_slice)\n",
        "\n",
        "        self.acoustic_imped_map = torch.rot90(self.acoustic_imped_map, 1, [0, 1])\n",
        "        diff_arr = torch.diff(self.acoustic_imped_map, dim=0)\n",
        "\n",
        "        diff_arr = torch.cat((torch.zeros(diff_arr.shape[1], dtype=torch.float32).unsqueeze(0).to(device='cuda'), diff_arr))\n",
        "\n",
        "        boundary_map =  -torch.exp(-(diff_arr**2)/alpha_coeff_boundary_map) + 1\n",
        "\n",
        "        boundary_map = torch.rot90(boundary_map, 3, [0, 1])\n",
        "\n",
        "        if self.params[\"debug\"]:\n",
        "           self.plot_fig(diff_arr, \"diff_arr\", False)\n",
        "           self.plot_fig(boundary_map, \"boundary_map\", True)\n",
        "\n",
        "        shifted_arr = torch.roll(self.acoustic_imped_map, -1, dims=0)\n",
        "        shifted_arr[-1:] = 0\n",
        "\n",
        "        sum_arr = self.acoustic_imped_map + shifted_arr\n",
        "        sum_arr[sum_arr == 0] = 1\n",
        "        div = diff_arr / sum_arr\n",
        "\n",
        "        refl_map = div ** 2\n",
        "        refl_map = torch.sigmoid(refl_map)      # 1 / (1 + (-refl_map).exp())\n",
        "        refl_map = torch.rot90(refl_map, 3, [0, 1])\n",
        "\n",
        "        if self.params[\"debug\"]: self.plot_fig(refl_map, \"refl_map\", True)\n",
        "\n",
        "        z_vals = self.render_rays(ct_slice.shape[0], ct_slice.shape[1])\n",
        "\n",
        "        if CLAMP_VALS:\n",
        "            self.clamp_map_ranges()\n",
        "\n",
        "        ret_list = self.rendering(ct_slice.shape[0], ct_slice.shape[1], z_vals=z_vals, refl_map=refl_map, boundary_map=boundary_map)\n",
        "\n",
        "        intensity_map  = ret_list[0]\n",
        "\n",
        "        if self.params[\"debug\"]:\n",
        "            self.plot_fig(intensity_map, \"intensity_map\", True)\n",
        "\n",
        "            result_list = [\"intensity_map\", \"attenuation_total\", \"reflection_total\",\n",
        "                            \"scatters_map\", \"scattering_probability\", \"border_convolution\",\n",
        "                            \"texture_noise\", \"b\", \"r\"]\n",
        "\n",
        "            for k in range(len(ret_list)):\n",
        "                result_np = ret_list[k]\n",
        "                if torch.is_tensor(result_np):\n",
        "                    result_np = result_np.detach().cpu().numpy()\n",
        "\n",
        "                if k==2:\n",
        "                    self.plot_fig(result_np, result_list[k], False)\n",
        "                else:\n",
        "                    self.plot_fig(result_np, result_list[k], True)\n",
        "                # print(result_list[k], \", \", result_np.shape)\n",
        "\n",
        "        intensity_map_masked = self.warp_img(intensity_map)\n",
        "        intensity_map_masked = torch.rot90(intensity_map_masked, 3)\n",
        "\n",
        "        if self.params[\"debug\"]:  self.plot_fig(intensity_map_masked, \"intensity_map_masked\", True)\n",
        "\n",
        "        return intensity_map_masked\n"
      ],
      "metadata": {
        "id": "NRLJBwammqoh",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Segmentation, Composition and US slicing code\n",
        "import scipy\n",
        "from torch import device\n",
        "# from torch.nn import OptimizedModule\n",
        "def dict_2_map(d: dict[list[uint8], uint8]) -> list[list[uint8]]:\n",
        "    map = [[] for _ in range(15)]\n",
        "\n",
        "    for k, v in d.items():\n",
        "        int_k = uint8(k)\n",
        "        map[v].append(int_k)\n",
        "\n",
        "    return map\n",
        "\n",
        "def batched(self, iterable, n):\n",
        "    it = iter(iterable)\n",
        "    while batch := tuple(islice(it, n)):\n",
        "        yield batch\n",
        "\n",
        "# Save time by initializing predictors once, instead of for each task\n",
        "def initialize_predictors(device: str = 'cuda',\n",
        "                        folds: list = (0,)) -> dict:\n",
        "    \"\"\"\n",
        "    Initialize nnUNetPredictor instances for each segmentation task.\n",
        "\n",
        "    Args:\n",
        "        device (str): Device to run predictions on ('cuda', 'cpu', 'mps').\n",
        "        use_folds (tuple): Fold indices to use for prediction.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary mapping task names to their respective nnUNetPredictor instances.\n",
        "    \"\"\"\n",
        "    # Define tasks\n",
        "    tasks = [(\"total\",\n",
        "            [291, 292, 293, 294, 295],\n",
        "            [\"Dataset291_TotalSegmentator_part1_organs_1559subj\",\n",
        "            \"Dataset292_TotalSegmentator_part2_vertebrae_1532subj\",\n",
        "            \"Dataset293_TotalSegmentator_part3_cardiac_1559subj\",\n",
        "            \"Dataset294_TotalSegmentator_part4_muscles_1559subj\",\n",
        "            \"Dataset295_TotalSegmentator_part5_ribs_1559subj\"],\n",
        "            [\"/v2.0.0-weights/Dataset291_TotalSegmentator_part1_organs_1559subj.zip\",\n",
        "            \"/v2.0.0-weights/Dataset292_TotalSegmentator_part2_vertebrae_1532subj.zip\",\n",
        "            \"/v2.0.0-weights/Dataset293_TotalSegmentator_part3_cardiac_1559subj.zip\",\n",
        "            \"/v2.0.0-weights/Dataset294_TotalSegmentator_part4_muscles_1559subj.zip\",\n",
        "            \"/v2.0.0-weights/Dataset295_TotalSegmentator_part5_ribs_1559subj.zip\"],\n",
        "            \"nnUNetTrainerNoMirroring\",\n",
        "            False),\n",
        "            (\"tissue_types\",\n",
        "            [481],\n",
        "            [\"Dataset481_tissue_1559subj\"],\n",
        "            [],\n",
        "            \"nnUNetTrainer\",\n",
        "            True),\n",
        "            (\"body\",\n",
        "            [299],\n",
        "            [\"Dataset299_body_1559subj\"],\n",
        "            [\"/v2.0.0-weights/Dataset299_body_1559subj.zip\"],\n",
        "            \"nnUNetTrainer\",\n",
        "            False)]\n",
        "\n",
        "    commercial_models_inv = {v: k for k, v in commercial_models.items()}\n",
        "    base_url = \"https://github.com/wasserth/TotalSegmentator/releases/download\"\n",
        "\n",
        "    # Get weights directory\n",
        "    weights_dir = get_weights_dir()\n",
        "\n",
        "    predictors = {}\n",
        "    for task_name, task_ids, paths, urls, trainer,with_license in tasks:\n",
        "        print(f\"INIT: {task_name} predictor\")\n",
        "        if with_license:\n",
        "            for i in range(len(task_ids)):\n",
        "                if paths[i] not in os.listdir(weights_dir):\n",
        "                    download_model_with_license_and_unpack(commercial_models_inv[task_ids[i]], weights_dir / paths[i])\n",
        "\n",
        "                # Initialize the predictor\n",
        "                predictor = nnUNetPredictor(\n",
        "                    tile_step_size=0.5,\n",
        "                    use_gaussian=True,\n",
        "                    use_mirroring=True,\n",
        "                    perform_everything_on_device=(device == 'cuda'),\n",
        "                    device=torch.device(device, 0),\n",
        "                    verbose=True,\n",
        "                    allow_tqdm=True\n",
        "                )\n",
        "                # Initialize from the trained model folder\n",
        "                predictor.initialize_from_trained_model_folder(\n",
        "                    str(weights_dir / paths[i] / (trainer + \"__nnUNetPlans__3d_fullres\")),\n",
        "                    use_folds=folds,\n",
        "                    checkpoint_name='checkpoint_final.pth'\n",
        "                )\n",
        "\n",
        "                predictors[task_ids[i]] = predictor\n",
        "\n",
        "        else:\n",
        "            for i in range(len(urls)):\n",
        "                if paths[i] not in os.listdir(weights_dir):\n",
        "                    download_url_and_unpack(base_url + urls[i], weights_dir / paths[i])\n",
        "\n",
        "                # Initialize the predictor\n",
        "                predictor = nnUNetPredictor(\n",
        "                    tile_step_size=0.5,\n",
        "                    use_gaussian=True,\n",
        "                    use_mirroring=True,\n",
        "                    perform_everything_on_device=(device == 'cuda'),\n",
        "                    device=torch.device(device, 0),\n",
        "                    verbose=True,\n",
        "                    allow_tqdm=True\n",
        "                )\n",
        "                # Initialize from the trained model folder\n",
        "                predictor.initialize_from_trained_model_folder(\n",
        "                    str(weights_dir / paths[i] / (trainer + \"__nnUNetPlans__3d_fullres\")),\n",
        "                    use_folds=folds,\n",
        "                    checkpoint_name='checkpoint_final.pth'\n",
        "                )\n",
        "                predictors[task_ids[i]] = predictor\n",
        "\n",
        "    return predictors\n",
        "\n",
        "def bin_erosion(kernel:torch.Tensor, padded:torch.Tensor, ret:torch.Tensor):\n",
        "    # Assumes stacked 3d and no normalization needed\n",
        "    i, hdx, idx, jdx = cuda.grid(4)\n",
        "\n",
        "    # Run kernel\n",
        "    window = padded[i,\n",
        "                    hdx-int((kernel.shape[0]-1) / 2):hdx+int((kernel.shape[0]-1) / 2),\n",
        "                    idx-int((kernel.shape[0]-1) / 2):idx+int((kernel.shape[0]-1) / 2),\n",
        "                    jdx-int((kernel.shape[0]-1) / 2):jdx+int((kernel.shape[0]-1) / 2)]\n",
        "    # TODO: does this also get JITed?\n",
        "    match = torch.all(kernel == window)\n",
        "    ret[i, hdx, idx, jdx] = 1 if match else 0\n",
        "\n",
        "def bin_dilation(kernel:torch.Tensor, padded:torch.Tensor, ret:torch.Tensor):\n",
        "    # Assumes stacked 3d and no normalization needed\n",
        "    i, hdx, idx, jdx = cuda.grid(4)\n",
        "\n",
        "    # Run kernel\n",
        "    window = padded[i,\n",
        "                    hdx-int((kernel.shape[0]-1) / 2):hdx+int((kernel.shape[0]-1) / 2),\n",
        "                    idx-int((kernel.shape[0]-1) / 2):idx+int((kernel.shape[0]-1) / 2),\n",
        "                    jdx-int((kernel.shape[0]-1) / 2):jdx+int((kernel.shape[0]-1) / 2)]\n",
        "    # TODO: does this also get JITed?\n",
        "    match = torch.any(kernel == window)\n",
        "    ret[i, hdx, idx, jdx] = 1 if match else 0\n",
        "\n",
        "class CT2US(torch.nn.Module):\n",
        "    def seg_predictor(self, imgs, properties, task, resamp_thr):\n",
        "        return self.predictors[task].predict_from_data_iterator(\n",
        "                                        self.iterator(self.predictors[task], imgs, properties),\n",
        "                                        save_probabilities=False,\n",
        "                                        num_processes_segmentation_export=resamp_thr\n",
        "                                    )\n",
        "\n",
        "        # Does not work for some reason, totalsegmentator returns zeros instead of labels\n",
        "    def seg_old(self, imgs, properties, task, resamp_thr):\n",
        "        ret = []\n",
        "\n",
        "        # TODO Get list from \"total\" labels and find matches in totalsegmentator\n",
        "        roi = [l for _, l in name2label[\"total\"].items()]\n",
        "        roi = np.concatenate(roi).tolist()\n",
        "        print(roi)\n",
        "        if task == \"total\":\n",
        "            for img in imgs:\n",
        "                ret.append(np.asarray(ts.totalsegmentator(\n",
        "                                    input=img,\n",
        "                                    task=task,\n",
        "                                    nr_thr_resamp=resamp_thr\n",
        "                                    # roi_subset=roi\n",
        "                                ).dataobj, dtype=np.uint8))\n",
        "\n",
        "        else:\n",
        "            for img in imgs:\n",
        "                ret.append(np.asarray(ts.totalsegmentator(\n",
        "                                    input=img,\n",
        "                                    task=task,\n",
        "                                    nr_thr_resamp=resamp_thr\n",
        "                                ).dataobj, dtype=np.uint8))\n",
        "\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def __init__(self, method: str = 'paths', device: str = 'cuda'):\n",
        "        super(CT2US, self).__init__()\n",
        "        methods = {'old', 'new', 'paths'}\n",
        "        if not method in methods:\n",
        "            raise KeyError(f\"Method not supported, choose from {methods}\")\n",
        "        else:\n",
        "            self.method = method\n",
        "\n",
        "        if device == 'cuda' and torch.cuda.is_available():\n",
        "            self.device = torch.device(device, torch.cuda.current_device())\n",
        "            self.m = cp\n",
        "        else:\n",
        "            self.device = torch.device('cpu')\n",
        "            self.m = np\n",
        "\n",
        "        self._dil_cuda = cuda.jit(bin_dilation)\n",
        "        self._er_cuda = cuda.jit(bin_erosion)\n",
        "        self._dil_cpu = njit(bin_dilation)\n",
        "        self._er_cpu = njit(bin_erosion)\n",
        "\n",
        "        if method == 'predictor':\n",
        "            predictors = initialize_predictors(device=device, folds=[0])\n",
        "            self.predictors = predictors\n",
        "            self.predictor_keys = predictors.keys()\n",
        "\n",
        "        segmentator = {\n",
        "            'new': lambda imgs, properties, task, resamp_thr: (NotImplementedError(\"WIP\")), #self.predict_tensor_iter,\n",
        "            'predictor': self.seg_predictor,\n",
        "            'old': self.seg_old\n",
        "        }\n",
        "\n",
        "        self.segmentator = segmentator[method]\n",
        "        us = {\n",
        "            'new': self.to_us_sim_new,\n",
        "            'predictor': self.to_us_sim_old,\n",
        "            'old': self.to_us_sim_old\n",
        "        }\n",
        "        self.us = us[method]\n",
        "\n",
        "        composer = {\n",
        "            'new': self.stacked_assemble_tid,\n",
        "            'predictor': self.assemble,\n",
        "            'old': self.assemble\n",
        "        }\n",
        "        self.composer = composer[method]\n",
        "\n",
        "        hparams = {\n",
        "            'debug' : False,\n",
        "            'device' : device\n",
        "        }\n",
        "\n",
        "        self.ultrasound_rendering = UltrasoundRendering(hparams, default_param=True)\n",
        "\n",
        "        with open(pthlib(this_folder, \"configs\", \"total_lmaps.json\")) as p:\n",
        "            total_lmap = dict(json.load(p))\n",
        "\n",
        "        with open(pthlib.joinpath(pthlib(this_folder), \"configs\", \"name2label.json\")) as n:\n",
        "            self.name2label = dict(json.load(n))\n",
        "\n",
        "        self.tmap = dict_2_map(total_lmap)\n",
        "\n",
        "    def bin_dilation(self, imgs:torch.Tensor, kernel_size:int ,iterations:int):\n",
        "        kernel = torch.ones((kernel_size, kernel_size, kernel_size), dtype=torch.uint8)\n",
        "        if imgs.is_cuda:\n",
        "            d_imgs = cuda.as_cuda_array(imgs.detach())\n",
        "            kernel = cuda.as_cuda_array(kernel.detach())\n",
        "            threadsperblock = (1, kernel_size, kernel_size, kernel_size)\n",
        "            blocks = (imgs.shape[0],\n",
        "                        np.ceil(imgs.shape[1] / threadsperblock[1]),\n",
        "                        np.ceil(imgs.shape[2] / threadsperblock[2]),\n",
        "                        np.ceil(imgs.shape[3] / threadsperblock[3]))\n",
        "            for _ in iterations:\n",
        "                ret = cuda.as_cuda_array(torch.zeros(imgs.shape, device=imgs.device).detach())\n",
        "                padded = cuda.as_cuda_array(\n",
        "                            imgs.to_padded_tensor(\n",
        "                                padding=0,\n",
        "                                output_size=(imgs.shape[0], imgs.shape[1] + 2, imgs.shape[2] + 2,imgs.shape[3] + 2)\n",
        "                            ).detach())\n",
        "                self._dil_cuda[blocks, threadsperblock](kernel, padded, ret)\n",
        "                d_imgs = ret\n",
        "        else:\n",
        "            d_imgs = imgs.detach().numpy()\n",
        "            kernel = kernel.detach()\n",
        "            for _ in iterations:\n",
        "                ret = torch.zeros(imgs.shape, device=imgs.device).detach()\n",
        "                padded = imgs.to_padded_tensor(\n",
        "                            padding=0,\n",
        "                            output_size=(imgs.shape[0], imgs.shape[1] + 2, imgs.shape[2] + 2,imgs.shape[3] + 2)\n",
        "                        ).detach()\n",
        "\n",
        "                self._dil_cpu(kernel, padded, ret)\n",
        "                d_imgs = ret\n",
        "\n",
        "        return d_imgs\n",
        "\n",
        "    def bin_erosion(self, imgs:torch.Tensor, kernel_size:int ,iterations:int):\n",
        "        kernel = torch.ones((kernel_size, kernel_size, kernel_size), dtype=torch.uint8)\n",
        "        if imgs.is_cuda:\n",
        "            d_imgs = cuda.as_cuda_array(imgs.detach())\n",
        "            kernel = cuda.as_cuda_array(kernel.detach())\n",
        "            threadsperblock = (1, kernel_size, kernel_size, kernel_size)\n",
        "            blocks = (imgs.shape[0],\n",
        "                        np.ceil(imgs.shape[1] / threadsperblock[1]),\n",
        "                        np.ceil(imgs.shape[2] / threadsperblock[2]),\n",
        "                        np.ceil(imgs.shape[3] / threadsperblock[3]))\n",
        "            for _ in iterations:\n",
        "                ret = cuda.as_cuda_array(torch.zeros(imgs.shape, device=imgs.device).detach())\n",
        "                padded = cuda.as_cuda_array(\n",
        "                            imgs.to_padded_tensor(\n",
        "                                padding=0,\n",
        "                                output_size=(imgs.shape[0], imgs.shape[1] + 2, imgs.shape[2] + 2,imgs.shape[3] + 2)\n",
        "                            ).detach())\n",
        "                self._er_cuda[blocks, threadsperblock](kernel, padded, ret)\n",
        "                d_imgs = ret\n",
        "        else:\n",
        "            d_imgs = imgs.detach().numpy()\n",
        "            kernel = kernel.detach()\n",
        "            for _ in iterations:\n",
        "                ret = torch.zeros(imgs.shape, device=imgs.device).detach()\n",
        "                padded = imgs.to_padded_tensor(\n",
        "                            padding=0,\n",
        "                            output_size=(imgs.shape[0], imgs.shape[1] + 2, imgs.shape[2] + 2,imgs.shape[3] + 2)\n",
        "                        ).detach()\n",
        "\n",
        "                self._er_cpu(kernel, padded, ret)\n",
        "                d_imgs = ret\n",
        "\n",
        "        return d_imgs\n",
        "\n",
        "    # Adapted from nnUNetPredictor\n",
        "    def iterator(self,\n",
        "                predictor: nnUNetPredictor,\n",
        "                imgs: list[np.ndarray],\n",
        "                properties: list[dict]):\n",
        "\n",
        "        # MAYBE: look at data_iterators.preprocess_fromnpy_save_to_queue for vstack use for ROI foreground masking\n",
        "\n",
        "        # pp = predictor.get_data_iterator_from_raw_npy_data(\n",
        "        #     imgs,\n",
        "        #     properties\n",
        "        # )\n",
        "\n",
        "        # preprocessor = predictor.configuration_manager.preprocessor_class(verbose=predictor.verbose)\n",
        "\n",
        "        # properties = {key: [i[key] for i in properties] for key in properties[0]}\n",
        "\n",
        "        # data, seg = preprocessor.run_case_npy(\n",
        "        #                 np.stack(imgs),\n",
        "        #                 None,\n",
        "        #                 properties,\n",
        "        #                 predictor.plans_manager,\n",
        "        #                 predictor.configuration_manager,\n",
        "        #                 predictor.dataset_json\n",
        "        #             )\n",
        "\n",
        "        # pass\n",
        "\n",
        "        preprocessor = predictor.configuration_manager.preprocessor_class(verbose=predictor.verbose)\n",
        "        for a, p in zip(imgs, properties):\n",
        "            data, seg = preprocessor.run_case_npy(a,\n",
        "                                                  None,\n",
        "                                                  p,\n",
        "                                                  predictor.plans_manager,\n",
        "                                                  predictor.configuration_manager,\n",
        "                                                  predictor.dataset_json)\n",
        "            yield {'data': torch.from_numpy(data).contiguous().pin_memory(), 'data_properties': p, 'ofile': None}\n",
        "\n",
        "    def convert_logits_to_segmentation(self, prediction, properties, predictor):\n",
        "        spacing_transposed = [properties['spacing'][i] for i in predictor.plans_manager.transpose_forward]\n",
        "        current_spacing = predictor.configuration_manager.spacing if \\\n",
        "            len(predictor.configuration_manager.spacing) == \\\n",
        "            len(properties['shape_after_cropping_and_before_resampling']) else \\\n",
        "            [spacing_transposed[0], *predictor.configuration_manager.spacing]\n",
        "        predicted_logits = predictor.configuration_manager.resampling_fn_probabilities(predicted_logits,\n",
        "                                                properties['shape_after_cropping_and_before_resampling'],\n",
        "                                                current_spacing,\n",
        "                                                [properties['spacing'][i] for i in predictor.plans_manager.transpose_forward])\n",
        "        # return value of resampling_fn_probabilities can be ndarray or Tensor but that does not matter because\n",
        "        # apply_inference_nonlin will convert to torch\n",
        "        predicted_probabilities = predictor.label_manager.apply_inference_nonlin(predicted_logits)\n",
        "        del predicted_logits\n",
        "        segmentation = predictor.label_manager.convert_probabilities_to_segmentation(predicted_probabilities)\n",
        "\n",
        "        # put segmentation in bbox (revert cropping)\n",
        "        segmentation_reverted_cropping = np.zeros(properties['shape_before_cropping'],\n",
        "                                                dtype=np.uint8 if len(predictor.label_manager.foreground_labels) < 255 else np.uint16)\n",
        "        slicer = tuple([slice(*i) for i in properties['bbox_used_for_cropping']])\n",
        "        segmentation_reverted_cropping[slicer] = segmentation\n",
        "        del segmentation\n",
        "\n",
        "        pass\n",
        "\n",
        "    # Adapted from nnUNetPredictor\n",
        "    def predict_tensor_iter(self,\n",
        "                        data_iterator,\n",
        "                        device: str = 'cuda') -> list[torch.tensor]:\n",
        "\n",
        "        r = []\n",
        "        for preprocessed in data_iterator:\n",
        "            asm = []\n",
        "            data = preprocessed['data']\n",
        "            print(f'perform_everything_on_device: {device==\"cuda\"}')\n",
        "            properties = preprocessed['data_properties']\n",
        "\n",
        "            for predictor in self.predictors.values():\n",
        "                old_threads = torch.get_num_threads()\n",
        "                # HYPERPARAMETER: number of threads to use for prediction\n",
        "                default_num_processes = 4\n",
        "                torch.set_num_threads(default_num_processes if default_num_processes < old_threads else old_threads)\n",
        "                prediction = None\n",
        "\n",
        "                for params in predictor.list_of_parameters:\n",
        "\n",
        "                    # messing with state dict names...\n",
        "                    # if not isinstance(predictor.network, OptimizedModule):\n",
        "                    #     predictor.network.load_state_dict(params)\n",
        "                    # else:\n",
        "                    #     predictor.network._orig_mod.load_state_dict(params)\n",
        "\n",
        "                    if prediction is None:\n",
        "                        prediction = predictor.predict_sliding_window_return_logits(data)\n",
        "                    else:\n",
        "                        prediction += predictor.predict_sliding_window_return_logits(data)\n",
        "\n",
        "                if len(predictor.list_of_parameters) > 1:\n",
        "                    prediction /= len(self.list_of_parameters)\n",
        "\n",
        "                prediction = self.convert_logits_to_segmentation(prediction, properties, predictor)\n",
        "\n",
        "            print(f'\\nDone with image of shape {data.shape}:')\n",
        "\n",
        "            # clear lru cache\n",
        "            compute_gaussian.cache_clear()\n",
        "            # clear device cache\n",
        "            if device.type == 'cuda':\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            r.append()\n",
        "\n",
        "        return [i.get()[0] for i in r]\n",
        "\n",
        "    def assemble(self,\n",
        "                task:str,\n",
        "                segs:list[np.ndarray],\n",
        "                bases:list[np.ndarray],\n",
        "                prev:list[np.ndarray]) -> list[np.ndarray]:\n",
        "\n",
        "        pr(\"ASSEMBLY STARTED\")\n",
        "        # Process total segmentation\n",
        "\n",
        "        if task == 'total':\n",
        "            for j in range(len(segs)):\n",
        "                for i in range(len(self.tmap)):\n",
        "                    if len(self.tmap[i]) > 0:  # if there are any keys for this value\n",
        "                        a = self.m.where(self.m.isin(self.m.asarray(segs[j], dtype=self.m.uint8), self.m.array(self.tmap[i])), self.m.uint8(i), self.m.uint8(0))\n",
        "                        prev[j] += a\n",
        "\n",
        "        if task == 'tissue_types':\n",
        "            for j in range(len(segs)):\n",
        "                t = self.m.asarray(segs[j])\n",
        "                prev[j][t == 1] = self.m.uint8(name2label[\"body\"][\"fat\"])\n",
        "                prev[j][t == 2] = self.m.uint8(name2label[\"body\"][\"fat\"])\n",
        "\n",
        "        if task == 'body':\n",
        "            for j in range(len(segs)):\n",
        "                t = self.m.asarray(segs[j])\n",
        "                body = cusci.binary_dilation(t == 1, iterations=1).astype(self.m.uint8)\n",
        "                body_inner = cusci.binary_erosion(t, iterations=3, brute_force=True).astype(self.m.uint8)\n",
        "                skin = body - body_inner\n",
        "\n",
        "                # Segment by density\n",
        "                # Roughly the skin density range. Made large to make segmentation not have holes\n",
        "                # (0 to 250 would have many small holes in skin)\n",
        "                density_mask = (bases[j] > -200) & (bases[j] < 250)\n",
        "                skin[~density_mask] = 0\n",
        "\n",
        "                # Fill holes\n",
        "                # skin = binary_closing(skin, iterations=1)  # no real difference\n",
        "                # skin = binary_dilation(skin, iterations=1)  # not good\n",
        "\n",
        "                mask, _ = cusci.label(skin)\n",
        "                counts = self.m.bincount(mask.flatten())  # number of pixels in each blob\n",
        "\n",
        "                # If only one blob (only background) abort because nothing to remove\n",
        "                if len(counts) > 1:\n",
        "                    remove = self.m.where((counts <= 10) | (counts > 30), True, False)\n",
        "                    remove_idx = self.m.nonzero(remove)[0]\n",
        "                    mask[self.m.isin(self.m.array(mask), remove_idx)] = 0\n",
        "                    mask[mask > 0] = 1\n",
        "\n",
        "                # Removing blobs\n",
        "                # End of snippet from totalsegmentator\n",
        "\n",
        "                dilation_kernel = self.m.ones(shape=(2, 2, 2))\n",
        "                skin = self.m.where(cusci.binary_dilation(skin == 1, structure=dilation_kernel), self.m.uint8(1), self.m.uint8(0))\n",
        "\n",
        "                prev[j][skin == 1] = self.m.uint8(name2label[\"body\"][\"skin\"])\n",
        "\n",
        "                tmp = prev[j].copy()\n",
        "                prev[j][tmp == 0] = self.m.uint8(name2label[\"body\"][\"bg\"])\n",
        "\n",
        "        pr(\"ASSEMBLY COMPLETED\")\n",
        "\n",
        "        del segs, bases\n",
        "\n",
        "        return prev\n",
        "\n",
        "    def stacked_assemble_tname(self, task:str,\n",
        "                segs:list[np.ndarray],\n",
        "                stacked_bases:list[np.ndarray],\n",
        "                prev: list[np.ndarray]) -> list[np.ndarray]:\n",
        "\n",
        "        pr(\"ASSEMBLY STARTED\")\n",
        "\n",
        "        # Process total segmentation\n",
        "        labels = prev\n",
        "\n",
        "        if task == \"total\":\n",
        "            stacked_totals = torch.stack([torch.as_tensor(segs[i], device=self.device) for i in range(stacked_bases.shape[0])], axis=0)\n",
        "\n",
        "            for i in range(len(self.tmap)):\n",
        "                if len(self.tmap[i]) > 0:  # if there are any keys for this value\n",
        "                    labels += torch.where(torch.isin(stacked_totals, torch.as_tensor(self.tmap[i], device=self.device)), np.uint8(i), np.uint8(0))\n",
        "\n",
        "        elif task == \"tissue_types\":\n",
        "            stacked_tissues = torch.stack([torch.as_tensor(segs[i], device=self.device) for i in range(stacked_bases.shape[0])], axis=0)\n",
        "\n",
        "            labels[stacked_tissues == 1] = np.uint8(name2label[\"body\"][\"fat\"])\n",
        "            labels[stacked_tissues == 2] = np.uint8(name2label[\"body\"][\"fat\"])\n",
        "\n",
        "        elif task == \"body\":\n",
        "            stacked_outers = torch.stack([torch.as_tensor(segs[i], device=self.device, dtype=torch.uint8) for i in range(stacked_bases.shape[0])], axis=0)\n",
        "\n",
        "            # Adapted code snippet from totalsegmentator\n",
        "            body = self.bin_dilation(stacked_outers == 1, kernel_size=3, iterations=1).astype(torch.uint8)\n",
        "            body_inner = self.bin_erosion(stacked_outers == 1, kernel_size=3, iterations=3).astype(torch.uint8)\n",
        "            skin = body - body_inner\n",
        "\n",
        "            # Segment by density\n",
        "            # Roughly the skin density range. Made large to make segmentation not have holes\n",
        "            # (0 to 250 would have many small holes in skin)\n",
        "            density_mask = (stacked_bases > -200) & (stacked_bases < 250)\n",
        "            skin[~density_mask] = 0\n",
        "\n",
        "            # Fill holes\n",
        "            # skin = binary_closing(skin, iterations=1)  # no real difference\n",
        "            # skin = binary_dilation(skin, iterations=1)  # not good\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                mask, _ = cusci.label(skin)\n",
        "            else:\n",
        "                mask, _ = scipy.ndimage.label(skin)\n",
        "\n",
        "            counts = torch.bincount(mask.flatten())  # number of pixels in each blob\n",
        "\n",
        "            # If only one blob (only background) abort because nothing to remove\n",
        "            if len(counts) > 1:\n",
        "                remove = torch.where((counts <= 10) | (counts > 30), True, False)\n",
        "                remove_idx = torch.nonzero(remove)[0]\n",
        "                mask[torch.isin(mask, remove_idx)] = 0\n",
        "                mask[mask > 0] = 1\n",
        "\n",
        "            # Removing blobs\n",
        "            # End of snippet from totalsegmentator\n",
        "            mask = torch.where(self.bin_dilation(mask == 1, kernel_size=3, iterations = 2), np.uint8(1), np.uint8(0))\n",
        "\n",
        "            labels[mask == 1] = np.uint8(name2label[\"body\"][\"skin\"])\n",
        "\n",
        "            tmp = labels.copy()\n",
        "            labels[tmp == 0] = np.uint8(name2label[\"body\"][\"bg\"])\n",
        "\n",
        "        pr(\"ASSEMBLY COMPLETED\")\n",
        "        del segs, bases\n",
        "\n",
        "        return labels\n",
        "\n",
        "    def stacked_assemble_tid(self, task:str,\n",
        "                segs:list[np.ndarray],\n",
        "                stacked_bases:list[np.ndarray],\n",
        "                prev: list[np.ndarray]) -> list[np.ndarray]:\n",
        "\n",
        "        pr(\"ASSEMBLY STARTED\")\n",
        "\n",
        "        # Process total segmentation\n",
        "        labels = prev\n",
        "\n",
        "        if task == \"total\":\n",
        "            stacked_totals = torch.stack([torch.as_tensor(segs[i], device=self.device) for i in range(stacked_bases.shape[0])], axis=0)\n",
        "\n",
        "            for i in range(len(self.tmap)):\n",
        "                if len(self.tmap[i]) > 0:  # if there are any keys for this value\n",
        "                    labels += torch.where(torch.isin(stacked_totals, torch.as_tensor(self.tmap[i], device=self.device)), np.uint8(i), np.uint8(0))\n",
        "\n",
        "        elif task == \"tissue_types\":\n",
        "            stacked_tissues = torch.stack([torch.as_tensor(segs[i], device=self.device) for i in range(stacked_bases.shape[0])], axis=0)\n",
        "\n",
        "            labels[stacked_tissues == 1] = np.uint8(name2label[\"body\"][\"fat\"])\n",
        "            labels[stacked_tissues == 2] = np.uint8(name2label[\"body\"][\"fat\"])\n",
        "\n",
        "        elif task == \"body\":\n",
        "            stacked_outers = torch.stack([torch.as_tensor(segs[i], device=self.device) for i in range(stacked_bases.shape[0])], axis=0)\n",
        "\n",
        "            # Adapted from TotalSegmentator\n",
        "            body = self.bin_dilation(stacked_outers == 1, kernel_size=3, iterations=1).astype(torch.uint8)\n",
        "            body_inner = self.bin_erosion(stacked_outers == 1, kernel_size=3, iterations=3).astype(torch.uint8)\n",
        "\n",
        "            skin = body - body_inner\n",
        "\n",
        "            # Segment by density\n",
        "            # Roughly the skin density range. Made large to make segmentation not have holes\n",
        "            # (0 to 250 would have many small holes in skin)\n",
        "            density_mask = (stacked_bases > -200) & (stacked_bases < 250)\n",
        "            skin[~density_mask] = 0\n",
        "\n",
        "            # Fill holes\n",
        "            # skin = binary_closing(skin, iterations=1)  # no real difference\n",
        "            # skin = binary_dilation(skin, iterations=1)  # not good\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                mask, _ = cusci.label(skin)\n",
        "            else:\n",
        "                mask, _ = scipy.ndimage.label(skin)\n",
        "\n",
        "            counts = torch.bincount(mask.flatten())  # number of pixels in each blob\n",
        "\n",
        "            # If only one blob (only background) abort because nothing to remove\n",
        "            if len(counts) > 1:\n",
        "                remove = torch.where((counts <= 10) | (counts > 30), True, False)\n",
        "                remove_idx = torch.nonzero(remove)[0]\n",
        "                mask[torch.isin(mask, remove_idx)] = 0\n",
        "                mask[mask > 0] = 1\n",
        "\n",
        "            # Removing blobs\n",
        "            # End of snippet from totalsegmentator\n",
        "\n",
        "            mask = torch.where(self.bin_dilation(mask == 1, kernel_size=3, iterations = 2), np.uint8(1), np.uint8(0))\n",
        "\n",
        "            labels[mask == 1] = np.uint8(name2label[\"body\"][\"skin\"])\n",
        "\n",
        "            tmp = labels.copy()\n",
        "            labels[tmp == 0] = np.uint8(name2label[\"body\"][\"bg\"])\n",
        "\n",
        "        pr(\"ASSEMBLY COMPLETED\")\n",
        "        del segs, bases\n",
        "\n",
        "        return labels\n",
        "\n",
        "    def to_us_sim_new(self, segs:list[np.ndarray], dest_us: list[str], step_size:int) -> list[list[np.ndarray]]:\n",
        "        pr(\"US SIMULATION STARTED\")\n",
        "\n",
        "        hparams = {\n",
        "            'debug' : False,\n",
        "            'device' : 'cuda'\n",
        "        }\n",
        "\n",
        "        us_r = UltrasoundRendering(params=hparams, default_param=True).to(hparams['device'])\n",
        "\n",
        "        transform = transforms.Compose([\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Resize([380, 380], transforms.InterpolationMode.NEAREST),\n",
        "                    transforms.CenterCrop((256)),\n",
        "                ])\n",
        "\n",
        "        results = []\n",
        "        for i in range(len(segs)):\n",
        "            us_images = []\n",
        "            labelmap = segs[i].get()\n",
        "            dest = pthlib(dest_us[i]).joinpath(\"slice_\")\n",
        "            os.makedirs(dest.parent, exist_ok=True)\n",
        "\n",
        "            for slice_idx in range(0, labelmap.shape[2], step_size):\n",
        "                slice_data = labelmap[:, :, slice_idx].astype('int64')\n",
        "                labelmap_slice = transform(slice_data).squeeze()\n",
        "\n",
        "                us_image = us_r(labelmap_slice)\n",
        "                us_images.append(us_image.cpu().numpy())\n",
        "\n",
        "                us_image_pil = transforms.ToPILImage()(us_image.cpu().squeeze())\n",
        "                us_image_pil.save(f\"{dest}_{slice_idx}.png\")\n",
        "\n",
        "            results.append(us_images)\n",
        "\n",
        "        print(\"US SIMULATION COMPLETED\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def to_us_sim_old(self, segs:list[np.ndarray], dest_us: list[str],  step_size:int) -> list[list[np.ndarray]]:\n",
        "        pr(\"US SIMULATION STARTED\")\n",
        "\n",
        "        hparams = {\n",
        "            'debug' : False,\n",
        "            'device' : 'cuda'\n",
        "        }\n",
        "\n",
        "        us_r = UltrasoundRendering(params=hparams, default_param=True).to(hparams['device'])\n",
        "\n",
        "        transform = transforms.Compose([\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Resize([380, 380], transforms.InterpolationMode.NEAREST),\n",
        "                    transforms.CenterCrop((256)),\n",
        "                ])\n",
        "\n",
        "        results = []\n",
        "        for i in range(len(segs)):\n",
        "            us_images = []\n",
        "            labelmap = segs[i].get()\n",
        "            dest = pthlib(dest_us[i]).joinpath(\"slice_\")\n",
        "            os.makedirs(dest.parent, exist_ok=True)\n",
        "\n",
        "            for slice_idx in range(0, labelmap.shape[2], step_size):\n",
        "                slice_data = labelmap[:, :, slice_idx].astype('int64')\n",
        "                labelmap_slice = transform(slice_data).squeeze()\n",
        "\n",
        "                us_image = us_r(labelmap_slice)\n",
        "                us_images.append(us_image.cpu().numpy())\n",
        "\n",
        "                us_image_pil = transforms.ToPILImage()(us_image.cpu().squeeze())\n",
        "                us_image_pil.save(f\"{dest}_{slice_idx}.png\")\n",
        "\n",
        "\n",
        "            results.append(us_images)\n",
        "\n",
        "        print(\"US SIMULATION COMPLETED\")\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "                    imgs: list[nifti1.Nifti1Image|np.ndarray|torch.Tensor],\n",
        "                    properties:list[dict],\n",
        "                    dest_label: list[str],\n",
        "                    dest_us: list[str],\n",
        "                    step_size: int,\n",
        "                    save_labels: bool,\n",
        "                ) -> list[list[np.ndarray]]:\n",
        "\n",
        "        if not self.method == 'old':\n",
        "            bases = torch.stack([\n",
        "                        torch.as_tensor(\n",
        "                            img.get_fdata(),\n",
        "                            device=self.device\n",
        "                        ) for img in imgs]\n",
        "                    ).cuda(self.device)\n",
        "\n",
        "            f_labels = torch.stack([\n",
        "                            torch.zeros(\n",
        "                                bases[i].shape,\n",
        "                                dtype=torch.uint8,\n",
        "                                device=self.device\n",
        "                            ) for i in range(bases.shape[0])],\n",
        "                            axis=0\n",
        "                        ).cuda(self.device)\n",
        "        else:\n",
        "            bases = [self.m.array(img.dataobj, dtype=self.m.float32) for img in imgs]\n",
        "            f_labels = [self.m.zeros(bases[idx].shape, dtype=self.m.uint8) for idx in range(len(imgs))]\n",
        "\n",
        "\n",
        "        if not self.method == \"old\":\n",
        "            tasks = list(self.predictor_keys)\n",
        "        else:\n",
        "            tasks = [\"total\", \"tissue_types\", \"body\"]\n",
        "\n",
        "        print(\"SEGMENTATING:\")\n",
        "\n",
        "        tmp = []\n",
        "        for idx in range(len(tasks)):\n",
        "            # tmp = predictors[idx].predict_from_list_of_npy_arrays(imgs, None, properties, None, 2, save_probabilities=False, num_processes_segmentation_export=2)\n",
        "\n",
        "            tmp.append(self.segmentator(imgs, properties, tasks[idx], 4))\n",
        "            print(\"SEG DONE!\")\n",
        "\n",
        "            # f_labels = self.composer(tasks[idx], tmp, stacked_bases, f_labels)\n",
        "\n",
        "        for idx in range(len(tasks)):\n",
        "            f_labels = self.composer(tasks[idx], tmp[idx], bases, f_labels)\n",
        "\n",
        "        us = self.us(f_labels.copy(), dest_us, step_size)\n",
        "\n",
        "        if save_labels:\n",
        "            for idx in range(len(f_labels)):\n",
        "                SimpleITKIO().write_seg(f_labels[idx].get().transpose(2, 1, 0), dest_label[idx], properties[idx])\n",
        "                print(f\"SAVED TO '{dest_label[idx]}'\")\n",
        "\n",
        "        # r_us = self.us[self.method](f_labels, bases, dest_us, dest_label, step_size, save_labels)\n",
        "\n",
        "        # return f_labels, stacked_bases, dest_us, dest_label, step_size, save_labels\n",
        "        return \"Success\""
      ],
      "metadata": {
        "id": "FX05Q45ymtFo",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Acquire samples\n",
        "todo_dir = path.joinpath(this_folder, \"sample\")\n",
        "todo_dir.mkdir(exist_ok=True)\n",
        "\n",
        "if not any(todo_dir.iterdir()):\n",
        "    !wget -O /content/CT2US/sample/sample.zip \"https://www.dropbox.com/scl/fi/mvr3l7ndar1b36c441ht1/synapse_raw_test0.zip?rlkey=7bzc6r7aqs0eyyh0eaidhnn31&st=z58ym3b5&dl=1\"\n",
        "    !unzip '/content/CT2US/sample/sample.zip' -d '/content/CT2US/sample'\n",
        "    !rm '/content/CT2US/sample/sample.zip'\n"
      ],
      "metadata": {
        "id": "AFfrTd9cmz6j",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title gradio interface code\n",
        "import shutil\n",
        "import gradio as gr\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "absolute_path = str(pthlib(\"../ct2us/imgs\").resolve())\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def process(fileobjs: list, step_size:int, save_labels:bool=False, progress=gr.Progress()):\n",
        "    for f in glob.glob(f\"{absolute_path}/*\"):\n",
        "        os.remove(f)\n",
        "\n",
        "    for f in fileobjs:\n",
        "        shutil.copyfile(f.name, absolute_path + \"/\" + pthlib(f.name).name)\n",
        "        shutil.rmtree(f.name, ignore_errors=True)\n",
        "\n",
        "    local_dataset = CTDataset(\n",
        "        device=device,\n",
        "        img_dir=absolute_path,\n",
        "        method='old',\n",
        "        resample=None\n",
        "    )\n",
        "    batch_size = 1\n",
        "\n",
        "    ct_dataloader = DataLoader(local_dataset, batch_size=batch_size, collate_fn=local_dataset.collate_fn)\n",
        "\n",
        "    ct2us = CT2US(method=\"old\", device=device)\n",
        "\n",
        "    for data in progress.tqdm(ct_dataloader, total=len(ct_dataloader), desc=\"Processing CT Scans\"):\n",
        "        imgs, properties, dest_labels, dest_us = data\n",
        "        ct2us(imgs, properties, dest_labels, dest_us, step_size, save_labels)\n",
        "\n",
        "    return \"All done!\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "e_O_b9kjJ9id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run this for actual UI\n",
        "%%blocks\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# CT to US Simulation\")\n",
        "    gr.Markdown(\"This generates ultrasound images from CT scans by segmentating into tissue types, which are then used to simulate the corresponding images.\")\n",
        "    gr.Interface(fn=process,\n",
        "                inputs=[\n",
        "                   \"files\",\n",
        "                   gr.Slider(value=5, minimum=1, maximum=50, step=1, label=\"Step Size\"),\n",
        "                   \"checkbox\"\n",
        "                ],\n",
        "                outputs=[\"text\"])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JM2YKlKlKRXR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}